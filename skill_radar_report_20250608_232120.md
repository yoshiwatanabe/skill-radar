# Weekly Tech Trend Report
**Period:** Jun 8 - Jun 9, 2025

## 📋 Weekly Summary

This week's technology landscape focused on key developments across 5 major areas.

## 🔥 Top Trending Technologies

### 1. gpt (41 mentions)
**Key insight:** Growing interest in gpt with 5 related articles this week

**Learning recommendation:** Consider exploring gpt fundamentals and practical applications

### 2. azure (36 mentions)
**Key insight:** Growing interest in azure with 5 related articles this week

**Learning recommendation:** Consider exploring azure fundamentals and practical applications

### 3. llm (33 mentions)
**Key insight:** Growing interest in llm with 5 related articles this week

**Learning recommendation:** Consider exploring llm fundamentals and practical applications

### 4. chatgpt (33 mentions)
**Key insight:** Growing interest in chatgpt with 5 related articles this week

**Learning recommendation:** Consider exploring chatgpt fundamentals and practical applications

### 5. devops (24 mentions)
**Key insight:** Growing interest in devops with 5 related articles this week

**Learning recommendation:** Consider exploring devops fundamentals and practical applications

## 📚 Must-Read Articles

### 1. [Chat is this real?](https://v.redd.it/99s1ye1z9p5f1)
**Source:** Reddit-ChatGPT | **Relevance:** 100%

### 2. [I built a C# Deep Research Meta Agent](https://i.redd.it/hmmc792obp5f1.png)
**Source:** Reddit-dotnet | **Relevance:** 100%

just wanted to share something I worked on a recenlty that might help someone here especially if you’re interested in building AI agents using .NET and C#.

I recently entered the **Microsoft AI Agents Hackathon 2025**, and my project **Apollo** ended up winning **Best C# Agent**!

Its basically a similiar to most deep research implementations but its fully *'agentic'*

* Plans a multi-step information retrieval strategy
* Scrapes live web content (via [EXA.ai](http://exa.ai/) )
* Embeds + stores data in `pgvector` on Neon
* Synthesizes a detailed report using GPT-4.1 and Gemini 2.5 Pro
* All built with **Semantic Kernel**, **Kernel Memory**, and **.NET 9**

Built in just under 30 days, definately not the cleanest architecture,but I did my best to keep it readable and modular. There’s definitely a lot of room for improvement especially around memory management and dynamic agent behavior, but I hope it’s useful as a reference for anyone trying to build practical agent workflows in C#.

🔗 GitHub: [https://github.com/manasseh-zw/apollo](https://github.com/manasseh-zw/apollo)

winners announcement here : [https://aka.ms/agentshackwinners](https://aka.ms/agentshackwinners)

**Tags:** AI, GPT, AI Agent, AI Agents, C#

### 3. [DevOps Isn’t Just Pipelines—It’s Creating Environments Where Quality Can Emerge](https://www.reddit.com/r/devops/comments/1l69g0j/devops_isnt_just_pipelinesits_creating/)
**Source:** Reddit-devops | **Relevance:** 100%

In the DevOps world, we champion automation, CI/CD, and fast delivery. But what about the organizational conditions that make true quality sustainable?

My new post looks at the resistance to quality practices (tests, simple design, pair programming) and how it's often tied to:

* Short-term delivery pressure
* Team-level silos and lack of alignment
* Poor feedback loops

We need more than tools—we need cultures that enable trust, learning, and shared ownership.

Full post here: [https://www.eferro.net/2025/06/overcoming-resistance-and-creating-conditions-for-quality.html](https://www.eferro.net/2025/06/overcoming-resistance-and-creating-conditions-for-quality.html)

How are you addressing the “people and incentives” side of quality in your DevOps practices?

**Tags:** AI, ML, DevOps, CI/CD, .NET

### 4. [Which channels that talk about system design and all this stuff do you watch?](https://www.reddit.com/r/dotnet/comments/1l6qjvj/which_channels_that_talk_about_system_design_and/)
**Source:** Reddit-dotnet | **Relevance:** 90%

I looking for something that use Azure like example. I always find a channel with AWS and Java.

**Tags:** Azure, AWS, Java, System Design

### 5. [The last six months in LLMs, illustrated by pelicans on bicycles](https://simonwillison.net/2025/Jun/6/six-months-in-llms/)
**Source:** HackerNews | **Relevance:** 87%

**Tags:** LLM

### 6. [What's actually the state of AI? Is this the peak, plateau or just the beginning?](https://www.reddit.com/r/singularity/comments/1l6a4nf/whats_actually_the_state_of_ai_is_this_the_peak/)
**Source:** Reddit-singularity | **Relevance:** 81%

I understand that this topic comes up daily and that there is a lot of speculation and opinions. This sub is understandably more inclined to believe AGI and/or ASI is coming soon than other subs. I might use some technical terms wrong or the words AI or LLM too losely at times, but I believe I get my thoughts across anyways.

I am also one who believes in AI and its potential, but I am no expert.  I guess what I am trying to seek is a reasonable view, amongst all the noise and hype, and I turn to this sub as I know that there are a lot of experts and very knowledgeable people here. I know that no one working at OpenAI, Google Deepmind, Anthropic etc is gonna break an NDA and give us a full rundown of the current state.  But my questions are: What's actually the deal? What are we really looking at?

Although AI is here to stay and it might completely take over. There are a couple of options that I see.

1. It's overhyped. This brings hype, investments, money. No company want to get left behind, and more investments are good for the companies regardless.

2. It's real. This justifies the hype, investements and money. The top companies and governments are scrambling to become first and number one.

3. It's reached it's top for the foreseeable future. The available models for the public are already revolutionary as they are and are already changing the landscape of science, tech and society.

Also from my understanding there are 2 bottlenecks. Data and Compute. (I wanted to insert a - so much between these two sentences, but I will not for understandable reasons lol.)

The models are already trained on all the high quality information that is available, that is most of human made data ever produced. Some of the quality data that is untapped:

Peoples personal photo libraries.

Smart watches and biometric data.

Live video and gps from personal phones.

Both the vast amounts of data points and the possibility of a real time global view of the world. If all this is avaialable and possible to process in real time then we have future prediction machine on our hands.

And the problem as the internet gets filled with more and more AI-content the models train on other AI-generated data and it becomes a negative feedback loop.

As for data, 100s of billions of dollars are invested into energy production and use for AI. There might be some point of energy that is needed to overcome the bump. 

There might also be an energy/computation treshold. Lowering energy usage through better algorithms and having more compute available. I like to compare it to the Great filter theory in the Fermi Paradox. There is a certain point here that needs to be overcome. Maybe it's hypothesis or an actual mathematical/physical treshold that needs to be reached. What is it?

The potential third I can think of is the Architecture of the AI or LLM. How it is constructed programatically. Maybe it is here something needs to change to bring forth the next "jump" in capabilites.

I am also trying to prepare for the future and become as competent as possible. I know if ASI comes there's not that much you can do as a single individual. I am wondering whether I should become an AI-engineer, 5 year degree with a masters. Not to neccessarily become a researcher or work at the biggest tech companies. But to integrate AI and machine learning into processes, logistics and business systems. Would this still be a smart move in 2025, or is it too late?

**Tags:** AI, Machine Learning, LLM, OpenAI, Anthropic

### 7. [Rig upgraded to 8x3090](https://i.redd.it/7ios74ratn5f1.jpeg)
**Source:** Reddit-LocalLLaMA | **Relevance:** 79%

About 1 year ago I posted about a [4 x 3090 build](https://www.reddit.com/r/LocalLLaMA/comments/1bqxfc0/another_4x3090_build/).  This machine has been great for learning to fine-tune LLMs and produce synthetic data-sets.  However, even with deepspeed and 8B models, the maximum training  full fine-tune context length was about 2560 tokens per conversation.  Finally I decided to get some 16-&gt;8x8 lane splitters, some more GPUs and some more RAM.   Training  Qwen/Qwen3-8B (full fine-tune) with 4K context length completed success fully and without pci errors, and I am happy with the build.  The spec is like:

* Asrock Rack EP2C622D16-2T
* 8xRTX 3090 FE (192 GB VRAM total)
* Dual Intel Xeon 8175M
* 512 GB DDR4 2400
* EZDIY-FAB PCIE Riser cables
* Unbranded Alixpress PCIe-Bifurcation 16X to x8x8 
* Unbranded Alixpress open chassis

As the lanes are now split, each GPU has about half the bandwidth.  Even if training takes a bit longer, being able to full fine tune to a longer context window is worth it in my opinion.



 

**Tags:** AI, LLM, Go

### 8. [When you figure out it’s all just math:](https://i.redd.it/t7ko9eywrq5f1.jpeg)
**Source:** Reddit-LocalLLaMA | **Relevance:** 77%

### 9. [LLM reasoning models are now able to arrive at novel solutions to unpublished problems in higher mathematics](https://www.scientificamerican.com/article/inside-the-secret-meeting-where-mathematicians-struggled-to-outsmart-ai/)
**Source:** Reddit-singularity | **Relevance:** 76%

This is interesting because it specifically related to unpublished but solvable mathematics problems posed by professional mathematicians.

I highly recommend checking out the short article and reading about the experiment design, because it appears highly legit.

A major critique of prior LLMs was they only good at parroting previously solved/published  mathematics. Here they appear to learn a mathematical field within minutes, create toy problems to practice the solution, and then offer novel solutions that are checked and validated by mathematicians.

**Tags:** LLM, Go, REST, Git

### 10. [This photo is AI. Can you tell?](https://i.redd.it/y9jlmy2n6r5f1.png)
**Source:** Reddit-ChatGPT | **Relevance:** 76%

**Tags:** AI

## 🎯 This Week's Learning Focus

- Deep dive: gpt (estimated 4-6 hours) - Focus on practical implementation
- Deep dive: azure (estimated 4-6 hours) - Focus on practical implementation
- Deep dive: llm (estimated 4-6 hours) - Focus on practical implementation

## 📊 Technology Buzz Words

| Technology | Mentions |
|------------|----------|
| ai | 135 |
| go | 77 |
| api | 44 |
| gpt | 41 |
| azure | 36 |
| llm | 33 |
| chatgpt | 33 |
| git | 29 |
| rest | 27 |
| github | 25 |

---
*Generated on 2025-06-08 23:21 by SkillRadar*
