# Weekly Tech Trend Report
**Period:** Jun 8 - Jun 9, 2025

## 📋 Weekly Summary

This week's technology landscape focused on key developments across 5 major areas.

## 🔥 Top Trending Technologies

### 1. ai (131 mentions)
**Key insight:** Growing interest in ai with 5 related articles this week

**Learning recommendation:** Consider exploring ai fundamentals and practical applications

### 2. go (75 mentions)
**Key insight:** Growing interest in go with 5 related articles this week

**Learning recommendation:** Consider exploring go fundamentals and practical applications

### 3. api (43 mentions)
**Key insight:** Growing interest in api with 5 related articles this week

**Learning recommendation:** Consider exploring api fundamentals and practical applications

### 4. gpt (41 mentions)
**Key insight:** Growing interest in gpt with 5 related articles this week

**Learning recommendation:** Consider exploring gpt fundamentals and practical applications

### 5. azure (36 mentions)
**Key insight:** Growing interest in azure with 5 related articles this week

**Learning recommendation:** Consider exploring azure fundamentals and practical applications

## 📚 Must-Read Articles

### 1. [Chat is this real?](https://v.redd.it/99s1ye1z9p5f1)
**Source:** Reddit-ChatGPT | **Relevance:** 100%

### 2. [I built a C# Deep Research Meta Agent](https://i.redd.it/hmmc792obp5f1.png)
**Source:** Reddit-dotnet | **Relevance:** 100%

just wanted to share something I worked on a recenlty that might help someone here especially if you’re interested in building AI agents using .NET and C#.

I recently entered the **Microsoft AI Agents Hackathon 2025**, and my project **Apollo** ended up winning **Best C# Agent**!

Its basically a similiar to most deep research implementations but its fully *'agentic'*

* Plans a multi-step information retrieval strategy
* Scrapes live web content (via [EXA.ai](http://exa.ai/) )
* Embeds + stores data in `pgvector` on Neon
* Synthesizes a detailed report using GPT-4.1 and Gemini 2.5 Pro
* All built with **Semantic Kernel**, **Kernel Memory**, and **.NET 9**

Built in just under 30 days, definately not the cleanest architecture,but I did my best to keep it readable and modular. There’s definitely a lot of room for improvement especially around memory management and dynamic agent behavior, but I hope it’s useful as a reference for anyone trying to build practical agent workflows in C#.

🔗 GitHub: [https://github.com/manasseh-zw/apollo](https://github.com/manasseh-zw/apollo)

winners announcement here : [https://aka.ms/agentshackwinners](https://aka.ms/agentshackwinners)

**Tags:** AI, GPT, AI Agent, AI Agents, C#

### 3. [DevOps Isn’t Just Pipelines—It’s Creating Environments Where Quality Can Emerge](https://www.reddit.com/r/devops/comments/1l69g0j/devops_isnt_just_pipelinesits_creating/)
**Source:** Reddit-devops | **Relevance:** 100%

In the DevOps world, we champion automation, CI/CD, and fast delivery. But what about the organizational conditions that make true quality sustainable?

My new post looks at the resistance to quality practices (tests, simple design, pair programming) and how it's often tied to:

* Short-term delivery pressure
* Team-level silos and lack of alignment
* Poor feedback loops

We need more than tools—we need cultures that enable trust, learning, and shared ownership.

Full post here: [https://www.eferro.net/2025/06/overcoming-resistance-and-creating-conditions-for-quality.html](https://www.eferro.net/2025/06/overcoming-resistance-and-creating-conditions-for-quality.html)

How are you addressing the “people and incentives” side of quality in your DevOps practices?

**Tags:** AI, ML, DevOps, CI/CD, .NET

### 4. [The last six months in LLMs, illustrated by pelicans on bicycles](https://simonwillison.net/2025/Jun/6/six-months-in-llms/)
**Source:** HackerNews | **Relevance:** 86%

**Tags:** LLM

### 5. [Which channels that talk about system design and all this stuff do you watch?](https://www.reddit.com/r/dotnet/comments/1l6qjvj/which_channels_that_talk_about_system_design_and/)
**Source:** Reddit-dotnet | **Relevance:** 85%

I looking for something that use Azure like example. I always find a channel with AWS and Java.

**Tags:** Azure, AWS, Java, System Design

### 6. [What's actually the state of AI? Is this the peak, plateau or just the beginning?](https://www.reddit.com/r/singularity/comments/1l6a4nf/whats_actually_the_state_of_ai_is_this_the_peak/)
**Source:** Reddit-singularity | **Relevance:** 80%

I understand that this topic comes up daily and that there is a lot of speculation and opinions. This sub is understandably more inclined to believe AGI and/or ASI is coming soon than other subs. I might use some technical terms wrong or the words AI or LLM too losely at times, but I believe I get my thoughts across anyways.

I am also one who believes in AI and its potential, but I am no expert.  I guess what I am trying to seek is a reasonable view, amongst all the noise and hype, and I turn to this sub as I know that there are a lot of experts and very knowledgeable people here. I know that no one working at OpenAI, Google Deepmind, Anthropic etc is gonna break an NDA and give us a full rundown of the current state.  But my questions are: What's actually the deal? What are we really looking at?

Although AI is here to stay and it might completely take over. There are a couple of options that I see.

1. It's overhyped. This brings hype, investments, money. No company want to get left behind, and more investments are good for the companies regardless.

2. It's real. This justifies the hype, investements and money. The top companies and governments are scrambling to become first and number one.

3. It's reached it's top for the foreseeable future. The available models for the public are already revolutionary as they are and are already changing the landscape of science, tech and society.

Also from my understanding there are 2 bottlenecks. Data and Compute. (I wanted to insert a - so much between these two sentences, but I will not for understandable reasons lol.)

The models are already trained on all the high quality information that is available, that is most of human made data ever produced. Some of the quality data that is untapped:

Peoples personal photo libraries.

Smart watches and biometric data.

Live video and gps from personal phones.

Both the vast amounts of data points and the possibility of a real time global view of the world. If all this is avaialable and possible to process in real time then we have future prediction machine on our hands.

And the problem as the internet gets filled with more and more AI-content the models train on other AI-generated data and it becomes a negative feedback loop.

As for data, 100s of billions of dollars are invested into energy production and use for AI. There might be some point of energy that is needed to overcome the bump. 

There might also be an energy/computation treshold. Lowering energy usage through better algorithms and having more compute available. I like to compare it to the Great filter theory in the Fermi Paradox. There is a certain point here that needs to be overcome. Maybe it's hypothesis or an actual mathematical/physical treshold that needs to be reached. What is it?

The potential third I can think of is the Architecture of the AI or LLM. How it is constructed programatically. Maybe it is here something needs to change to bring forth the next "jump" in capabilites.

I am also trying to prepare for the future and become as competent as possible. I know if ASI comes there's not that much you can do as a single individual. I am wondering whether I should become an AI-engineer, 5 year degree with a masters. Not to neccessarily become a researcher or work at the biggest tech companies. But to integrate AI and machine learning into processes, logistics and business systems. Would this still be a smart move in 2025, or is it too late?

**Tags:** AI, Machine Learning, LLM, OpenAI, Anthropic

### 7. [Rig upgraded to 8x3090](https://i.redd.it/7ios74ratn5f1.jpeg)
**Source:** Reddit-LocalLLaMA | **Relevance:** 79%

About 1 year ago I posted about a [4 x 3090 build](https://www.reddit.com/r/LocalLLaMA/comments/1bqxfc0/another_4x3090_build/).  This machine has been great for learning to fine-tune LLMs and produce synthetic data-sets.  However, even with deepspeed and 8B models, the maximum training  full fine-tune context length was about 2560 tokens per conversation.  Finally I decided to get some 16-&gt;8x8 lane splitters, some more GPUs and some more RAM.   Training  Qwen/Qwen3-8B (full fine-tune) with 4K context length completed success fully and without pci errors, and I am happy with the build.  The spec is like:

* Asrock Rack EP2C622D16-2T
* 8xRTX 3090 FE (192 GB VRAM total)
* Dual Intel Xeon 8175M
* 512 GB DDR4 2400
* EZDIY-FAB PCIE Riser cables
* Unbranded Alixpress PCIe-Bifurcation 16X to x8x8 
* Unbranded Alixpress open chassis

As the lanes are now split, each GPU has about half the bandwidth.  Even if training takes a bit longer, being able to full fine tune to a longer context window is worth it in my opinion.



 

**Tags:** AI, LLM, Go

### 8. [Life before ci/cd](https://www.reddit.com/r/devops/comments/1l6djk5/life_before_cicd/)
**Source:** Reddit-devops | **Relevance:** 75%

Hello,

Can anyone explain how life was before ci/cd pipeline. 

I understand developers and operations team were so separate. 

So how the DevOps culture now make things faster!? Is it like developer doesn’t need to depend on operations team to deploy his application ? And operations team focus on SRE ? Is my understanding correct ?

**Tags:** AI, SRE, DevOps, CI/CD

### 9. [When you figure out it’s all just math:](https://i.redd.it/t7ko9eywrq5f1.jpeg)
**Source:** Reddit-LocalLLaMA | **Relevance:** 75%

### 10. [I asked AI to pick a watermelon for me....and omg!](https://www.reddit.com/gallery/1l61r9u)
**Source:** Reddit-ChatGPT | **Relevance:** 75%

I asked AI to help me pick a watermelon… 

Told it I wanted one that was sweet and crunchy. Not mushy or overripe, the perfect watermelon basically. 

I started uploading pictures of different watermelons at the store… and it would literally tell me “pass” or “go.” Once it saw the right one it said "bingo go with that one..." 

So I bought it. Came home. Cut it up.

And it’s perfect...

10/10. Would trust it with my fruit again

**Tags:** AI, Rust, Go

## 🎯 This Week's Learning Focus

- Deep dive: ai (estimated 4-6 hours) - Focus on practical implementation
- Deep dive: go (estimated 4-6 hours) - Focus on practical implementation
- Deep dive: api (estimated 4-6 hours) - Focus on practical implementation

## 📊 Technology Buzz Words

| Technology | Mentions |
|------------|----------|
| ai | 131 |
| go | 75 |
| api | 43 |
| gpt | 41 |
| azure | 36 |
| chatgpt | 35 |
| llm | 31 |
| rest | 26 |
| git | 24 |
| devops | 24 |

---
*Generated on 2025-06-08 20:16 by SkillRadar*
